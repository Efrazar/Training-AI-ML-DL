{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f9f0d99-cb75-4510-9635-52542982aac9",
   "metadata": {},
   "source": [
    "# Principles of Deep Learning\n",
    "\n",
    "Thinking about deep learning as a biological process of adaptation can be very helpful. Let's break down how a neural network learns, which, at its core, is a cycle of guessing, checking, and correcting.\n",
    "\n",
    "Think of it like this: you're trying to teach a cell culture to respond to a new growth factor. You expose it, measure a response (e.g., protein expression), see how far off it is from the desired response, and then tweak the signaling pathway to get closer next time. The deep learning training loop is a mathematical formalization of that exact process.\n",
    "\n",
    "This entire cycle is often called training the model. It consists of four key steps that are repeated over and over.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d6eeec-6234-4f89-b8a1-54ca129641ec",
   "metadata": {},
   "source": [
    "## 1. The Forward Pass: Making a Prediction ‚û°Ô∏è\n",
    "The forward pass (or forward propagation) is the process of your neural network making a guess. You take your input data‚Äîsay, a set of gene expression values from an RNA-seq experiment‚Äîand pass it forward through the network's layers.\n",
    "\n",
    "Each layer in the network is composed of \"neurons\" (nodes). A neuron receives inputs, performs a simple calculation, and passes the result to the next layer. This calculation is typically a weighted sum of its inputs, plus a value called a bias, which is then fed into an activation function.\n",
    "\n",
    "    Weighted Sum: This is just like a linear regression: z=(w1‚Äãx1‚Äã+w2‚Äãx2‚Äã+‚Ä¶)+b. The weights (w) are the most important part; they are the internal parameters the network will \"learn.\" Initially, they are random. They represent the strength of the connection between neurons, analogous to synaptic strength.\n",
    "\n",
    "    Activation Function: This function introduces non-linearity, which is critical. A biological neuron either fires or it doesn't‚Äîit's not a simple linear switch. An activation function like a ReLU (Rectified Linear Unit) or Sigmoid mimics this. It takes the weighted sum (z) and decides what the neuron's output should be.\n",
    "\n",
    "This process continues layer by layer until the final layer produces an output‚Äîthe network's prediction (y^‚Äã). For example, it might output a single number between 0 and 1, representing the probability that your input gene expression profile corresponds to a \"cancerous\" cell.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc1d4f0-ae2d-4155-a916-56f6e889e65a",
   "metadata": {},
   "source": [
    "## 2. Loss Calculation: Quantifying the Error üìâ\n",
    "Now that the network has made a prediction (y^‚Äã), we need to tell it how wrong it was. The loss function (also called a cost function or objective function) does exactly this. It compares the network's prediction (y^‚Äã) with the ground truth (the correct label, y), which you know from your experimental data.\n",
    "\n",
    "The result is a single number called the loss. A high loss means the prediction was terrible; a low loss means it was pretty good.\n",
    "\n",
    "A common loss function for classification tasks (like \"cancerous\" vs. \"healthy\") is Binary Cross-Entropy. The formula looks a bit intimidating, but the concept is simple: it heavily penalizes predictions that are confidently wrong.\\\n",
    "Loss=‚àí[ylog(y^‚Äã)+(1‚àíy)log(1‚àíy^‚Äã)]\n",
    "\n",
    "If the true label y=1 and your model predicts y^‚Äã=0.9 (90% confident it's 1), the loss is small. If it predicts y^‚Äã=0.1, the loss is huge!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99701634-a851-4083-b671-a343f5109218",
   "metadata": {},
   "source": [
    "### Example of Loss calculation using Binary Cross-Entropy \n",
    "\n",
    "Formula:\n",
    "        Loss = -[y*log(y_hat) + (1-y_hat)*log(1-y_hat)]\n",
    "\n",
    "Predictions: 0.9 and 0.1.\n",
    "Ground truth = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00af9cc7-64e5-4ea0-bd57-391f726a8fe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss of predition 0.9 = 0.11\n",
      "loss of predition 0.1 = 2.40\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Apply loss function\n",
    "y9 = -(1*np.log(0.9))\n",
    "y1 = -(1*np.log(0.1) + (1-0.1)*np.log(1-0.1))\n",
    "\n",
    "print(f'loss of predition 0.9 = {y9:.2f}')\n",
    "print(f'loss of predition 0.1 = {y1:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be981468-e76d-4c5f-b848-1706ea3c31de",
   "metadata": {},
   "source": [
    "## 3. Backward Propagation: Assigning Blame ‚¨ÖÔ∏è\n",
    "This is the magic of deep learning. Now that we have the loss, we need to figure out which weights in the network were most responsible for the error and how to change them to do better next time. This process is called backward propagation or backprop.\n",
    "\n",
    "Using calculus (specifically the chain rule), backprop calculates the gradient of the loss with respect to every single weight and bias in the network. A gradient is essentially a vector that points in the direction of the steepest ascent of the loss function. Therefore, if we move the weights in the opposite direction of the gradient, we will decrease the loss.\n",
    "\n",
    "Think of it as a \"blame assignment\" algorithm. It starts from the loss and works its way backward through the network, layer by layer, calculating how much each weight contributed to the final error. A weight that had a large impact on the wrong output will get a large gradient.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4ce20d-e090-4efe-b530-fff8a0b8780c",
   "metadata": {},
   "source": [
    "## 4. Iteration and Optimization: Updating the Model üõ†Ô∏è\n",
    "he final step is to actually update the weights and biases using the gradients we just calculated. This is handled by an optimizer. The most fundamental optimizer is Stochastic Gradient Descent (SGD).\n",
    "\n",
    "The update rule is simple:\n",
    "new_weight=old_weight‚àí(learning_rate√ógradient)\n",
    "\n",
    "The learning rate is a small number (e.g., 0.01) that controls how big of a step we take. It's a critical hyperparameter:\n",
    "\n",
    "    Too high, and you might overshoot the optimal weights, like a clumsy scientist adding way too much reagent.\n",
    "\n",
    "    Too low, and the model will learn excruciatingly slowly.\n",
    "\n",
    "This entire four-step cycle‚Äîforward pass, loss calculation, backprop, and weight update‚Äîis one iteration. We repeat this process many times, usually by feeding the model data in small batches (e.g., 32 or 64 samples at a time). One full pass through the entire training dataset is called an epoch. After many epochs, the network's weights are finely tuned, and the loss is minimized. The model has learned!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2a7f02-70e4-4842-98de-20a2fa7c2084",
   "metadata": {},
   "source": [
    "## 5. Hands-On Example with PyTorch\n",
    "\n",
    "Here is a simple, fully-commented script that demonstrates this entire process. We'll create a toy dataset where we try to classify points into two categories based on their (x, y) coordinates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb420685-92b1-4676-b5a5-b9054a4fa675",
   "metadata": {},
   "source": [
    "### LEARNING SOME PYTORCH BASICS\n",
    "The torch.nn module provides everything you need to assemble a neural network. The key idea is modularity. You build a complex network by stacking together simpler pieces, much like building with LEGO blocks.\n",
    "\n",
    "The main components you'll use from torch.nn are:\n",
    "\n",
    "    Layers: These are the fundamental building blocks of a network. They take in tensors (the data), perform a specific mathematical operation, and pass the resulting tensors to the next layer. The most common ones are:\n",
    "\n",
    "        nn.Linear(in_features, out_features): A fully connected layer, which applies a linear transformation: y=Wx+b.\n",
    "\n",
    "        nn.Conv2d(...): A convolutional layer, essential for image processing tasks.\n",
    "\n",
    "        nn.RNN(...), nn.LSTM(...): Recurrent layers used for sequence data like text or time series.\n",
    "\n",
    "    Activation Functions: These introduce non-linearity into your network, allowing it to learn complex patterns. Without them, a neural network would just be a very deep linear model. Common examples include:\n",
    "\n",
    "        nn.ReLU(): Rectified Linear Unit. It's simple and effective: f(x)=max(0,x).\n",
    "\n",
    "        nn.Sigmoid(): Squeezes values between 0 and 1, often used for binary classification outputs.\n",
    "\n",
    "        nn.Softmax(): Used for multi-class classification to convert raw scores into probabilities.\n",
    "\n",
    "    Loss Functions: These measure how far your model's prediction is from the actual target (the ground truth). The goal of training is to minimize this value.\n",
    "\n",
    "        nn.MSELoss(): Mean Squared Error, common for regression tasks.\n",
    "\n",
    "        nn.CrossEntropyLoss(): The standard choice for classification tasks.\n",
    "\n",
    "    Containers: These help you organize your layers into a cohesive model.\n",
    "\n",
    "        nn.Module: The base class for all neural network modules. When you define your own network, you'll create a class that inherits from nn.Module. This is crucial because it automatically tracks all the learnable parameters (weights and biases) within your layers.\n",
    "\n",
    "        nn.Sequential: A container that allows you to chain layers and operations together in a simple, linear stack."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d188d6fb-7c31-4e8b-99a6-ecb75378dc4e",
   "metadata": {},
   "source": [
    "#### A Small Example: Building a Partial Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1fbe156a-4c9b-41e7-8d30-60a64de462be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 1. Define the network architecture by creating a class that inherits from nn.Module.\n",
    "class SimpleNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        # Call the constructor of the parent class (nn.Module)\n",
    "        super(SimpleNetwork, self).__init__()\n",
    "\n",
    "        # Define the layers you will use. nn.Module will track their parameters.\n",
    "        self.layer1 = nn.Linear(in_features=10, out_features=5) # Input: 10, Hidden: 5\n",
    "        self.activation = nn.ReLU()\n",
    "        self.layer2 = nn.Linear(in_features=5, out_features=1) # Hidden: 5, Output: 1\n",
    "\n",
    "    # The forward() method defines the sequence of operations.\n",
    "    # This is where you specify how data flows through the network.\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.layer2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1bab58f2-34ad-438c-bc0b-486134176d69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Simple Network\" model architecture:\n",
      "SimpleNetwork(\n",
      "  (layer1): Linear(in_features=10, out_features=5, bias=True)\n",
      "  (activation): ReLU()\n",
      "  (layer2): Linear(in_features=5, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 2. Create an instance of the network.\n",
    "model = SimpleNetwork()\n",
    "print('\"Simple Network\" model architecture:')\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49c7be7d-c933-49be-8858-2755ebd6fee0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.7015,  1.4560, -1.5479, -0.2399, -0.5083,  0.8717,  0.0032, -0.8095,\n",
       "          1.3964,  0.5748],\n",
       "        [-1.3217,  0.8034,  1.8372,  0.4521, -0.9674,  0.5541, -0.8576,  0.0545,\n",
       "         -0.7165, -1.3152],\n",
       "        [ 0.4965, -0.3070,  0.9838, -0.5722, -0.9212,  1.3286, -0.0956,  0.1977,\n",
       "         -0.5860,  2.6606]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. Create some dummy input data.\n",
    "# Let's pretend we have a batch of 3 samples, each with 10 features.\n",
    "input_tensor = torch.randn(3,10)\n",
    "input_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f0d825a-3471-4d08-9b7a-22331b297d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Pass the data through the model (this calls the forward() method).\n",
    "output = model(input_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b61175d5-4d8b-4d0e-ba0f-25bf2eba0a08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Shape of the input tensor: torch.Size([3, 10])\n",
      "\n",
      "Shape of the output tensor: torch.Size([3, 1])\n",
      "\n",
      "Output from the model: \n",
      " tensor([[ 0.0202],\n",
      "        [-0.1091],\n",
      "        [-0.0347]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print('\\nShape of the input tensor:', input_tensor.shape)\n",
    "print('\\nShape of the output tensor:', output.shape)\n",
    "print('\\nOutput from the model:', '\\n', output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e65242d-261e-4439-851b-5016b40b7b78",
   "metadata": {},
   "source": [
    "### Now move to a complete example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7c019040-e21a-4441-8fbb-ff413012ad13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground truth values (6 cells with a type 0 or 1: \n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.]])\n",
      "Cell type tensor shape: \n",
      "torch.Size([6, 1])\n",
      "\n",
      "Relative expression of gene A and B per cell: \n",
      "tensor([[1., 1.],\n",
      "        [1., 4.],\n",
      "        [2., 2.],\n",
      "        [4., 3.],\n",
      "        [5., 4.],\n",
      "        [6., 5.]])\n",
      "Genes expression tensor shape: \n",
      "torch.Size([6, 2])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# --- 0. Data Preparation ---\n",
    "# Let's create some synthetic data. Imagine these are two features from your cells.\n",
    "# We want to classify a cell as type 0 or type 1.\n",
    "# Features (e.g., expression of Gene A, expression of Gene B)\n",
    "\n",
    "X = torch.tensor([[1.0, 1.0], [1.0, 4.0], [2.0, 2.0], [4.0, 3.0], [5.0, 4.0], [6.0, 5.0]], dtype=torch.float32)\n",
    "# Labels (Ground Truth: Type 0 or Type 1)\n",
    "Y = torch.tensor([[0], [0], [0], [1], [1], [1]], dtype=torch.float32)\n",
    "\n",
    "print(f'Ground truth values (6 cells with a type 0 or 1: \\n{Y}')\n",
    "print(f'Cell type tensor shape: \\n{Y.shape}')\n",
    "print(f'\\nRelative expression of gene A and B per cell: \\n{X}')\n",
    "print(f'Genes expression tensor shape: \\n{X.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ca22f70d-73a5-4301-8c2d-f4c8e342811f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define the Model Architecture ---\n",
    "# A very simple neural network with one input layer, one hidden layer, and one output layer.\n",
    "\n",
    "class SimpleNeuralNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNeuralNet, self).__init__()\n",
    "        self.features1 = nn.Linear(2, 4) # Input features=2, hidden neurons=4\n",
    "        self.relu = nn.ReLU() # Non-linear activation function\n",
    "        self.features2 = nn.Linear(4, 1) # Hidden neurons=4, output classes=1\n",
    "        self.sigmoid = nn.Sigmoid() # Squashes output to a probability (0 to 1)\n",
    "\n",
    "    # The forward() method defines the sequence of operations.\n",
    "    def forward(self, x):\n",
    "        out = self.features1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.features2(out)\n",
    "        out = self.sigmoid(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0d981f25-11cb-4f23-a553-350ce09a9aee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Simple Neural Network\" model architecture:\n",
      "SimpleNeuralNet(\n",
      "  (features1): Linear(in_features=2, out_features=4, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (features2): Linear(in_features=4, out_features=1, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n",
      "\n",
      "Initial (random) prediction for first data point: 0.5624\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Pass the data through the model (this calls the forward() method).\n",
    "class_model = SimpleNeuralNet()\n",
    "print('\"Simple Neural Network\" model architecture:')\n",
    "print(class_model)\n",
    "print(f\"\\nInitial (random) prediction for first data point: {class_model(X[0]).item():.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c55144c6-c19e-4e36-aafc-edb208021059",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define Loss and Optimizer ---\n",
    "learning_rate = 0.01\n",
    "loss_function = nn.BCELoss() # Binary Cross-Entropy Loss\n",
    "optimizer = torch.optim.SGD(class_model.parameters(), lr=learning_rate) # Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3ffb4aba-d129-4c44-99c6-d5293e622a1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10000/100000], Loss: 0.0002\n",
      "Epoch [20000/100000], Loss: 0.0002\n",
      "Epoch [30000/100000], Loss: 0.0002\n",
      "Epoch [40000/100000], Loss: 0.0002\n",
      "Epoch [50000/100000], Loss: 0.0002\n",
      "Epoch [60000/100000], Loss: 0.0002\n",
      "Epoch [70000/100000], Loss: 0.0001\n",
      "Epoch [80000/100000], Loss: 0.0001\n",
      "Epoch [90000/100000], Loss: 0.0001\n",
      "Epoch [100000/100000], Loss: 0.0001\n",
      "\n",
      "--- Training Finished ---\n"
     ]
    }
   ],
   "source": [
    "# --- The Training Loop ---\n",
    "num_epochs = 100000\n",
    "\n",
    "    # ----------------------------------------------\n",
    "    #  THE FOUR STEPS OF DEEP LEARNING IN A FOR LOOP\n",
    "    # ----------------------------------------------\n",
    "\n",
    "for epoch in (range(num_epochs)):\n",
    "    # 1. FORWARD PASS: Make a prediction\n",
    "    predictions = class_model(X)\n",
    "    \n",
    "    # 2. LOSS CALCULATION: Quantify the error\n",
    "    loss = loss_function(predictions, Y)\n",
    "    \n",
    "    # 3. BACKWARD PROPAGATION: Calculate gradients (\"assign blame\")\n",
    "    # First, clear old gradients from the previous iteration\n",
    "    optimizer.zero_grad()\n",
    "    # Now, perform backpropagation\n",
    "    loss.backward()\n",
    "\n",
    "    # 4. UPDATE WEIGHTS: Adjust the model's parameters\n",
    "    optimizer.step()\n",
    "\n",
    "    # Show the iteration process every 10 epochs\n",
    "    if (epoch + 1) % 10000 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "print(\"\\n--- Training Finished ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd628809-362a-45ae-9395-ed25533213de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python(PyTorch)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
